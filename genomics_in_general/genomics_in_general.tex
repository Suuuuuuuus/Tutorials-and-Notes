\documentclass[UTF8]{book}
%\usepackage{ctex}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{makeidx}
\usepackage{enumitem}
\usepackage{rotating} 
\usepackage{yhmath}
\usepackage{textcomp,booktabs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{gensymb}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{pifont}
\usepackage[all,pdf]{xy}
\usepackage{exscale}
\usepackage{blindtext}
\usepackage{hyperref}
\hypersetup{
colorlinks=true,
linkcolor=black
}
\usepackage{nameref}
\usepackage{relsize}
\usepackage{titlesec}
\usepackage{ifthen}
\usepackage{array}
\usepackage[flushleft]{threeparttable}
\usepackage{diagbox}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{ulem}
\usepackage{xcolor}
\usepackage{color}
\usepackage{mathptmx}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, left=2.54cm,right=2.54cm,top=2.54cm,bottom=2.54cm}
%\geometry{b5paper, left=1.6cm,right=2cm,top=2cm,bottom=2cm}
\usepackage{mathrsfs}
\usepackage{tikz,tkz-euclide}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{shapes,arrows}
\usepackage{esvect}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % clear all fields 
\cfoot{}
\fancyhead[LE,RO]{\thepage} 
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\linespread{1.4}
\date{}
\graphicspath{ {Graphs} }
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  backgroundcolor=\color{gray!20!white}
}
\definecolor{codegray}{gray}{0.8}
\newcommand{\code}[1]{\colorbox{codegray}{\texttt{#1}}}
%通用：
\newcounter{mylabelcounter}
\makeatletter
\newcommand{\labeltext}[2]{%
#1\refstepcounter{mylabelcounter}%
\immediate\write\@auxout{%
  \string\newlabel{#2}{{1}{\thepage}{{\unexpanded{#1}}}{mylabelcounter.\number\value{mylabelcounter}}{}}%
}%
}
%Xsum
\DeclareFontFamily{U} {cmex}{}
\DeclareFontShape{U}{cmex}{m}{n}{
  <-6> cmex5
  <6-7> cmex6
  <7-8> cmex7
  <8-9> cmex8
  <9-10> cmex9
  <10-12> cmex10
  <12-> cmex12}{}
\DeclareSymbolFont{Xcmex} {U} {cmex}{m}{n}
\DeclareMathSymbol{\Xdsum}{\mathop}{Xcmex}{88}
\DeclareMathSymbol{\Xtsum}{\mathop}{Xcmex}{80}
\DeclareMathOperator*{\Xsum}{\mathchoice{\Xdsum}{\Xtsum}{\Xtsum}{\Xtsum}}
%Xsum
%choice
\newcommand{\fourch}[4]{%~\hfill(\qquad)\\
\begin{tabular}{*{4}{@{}p{0.25\textwidth}}}(A)~#1 & (B)~#2 & (C)~#3 & (D)~#4\end{tabular}}
\newcommand{\twoch}[4]{%~\hfill(\qquad)\\
\begin{tabular}{*{2}{@{}p{0.5\textwidth}}}(A)~#1 & (B)~#2\end{tabular}\\\begin{tabular}{*{2}{@{}p{0.5\textwidth}}}(C)~#3 & (D)~#4\end{tabular}}
\newcommand{\onech}[4]{%~\hfill(\qquad)\\
(A)~#1 \\ (B)~#2 \\ (C)~#3 \\ (D)~#4}
 
\newlength\widthcha
\newlength\widthchb
\newlength\widthchc
\newlength\widthchd
\newlength\widthch
\newlength\tabmaxwidth
\setlength\tabmaxwidth{1\textwidth}
\newlength\fourthtabwidth
\setlength\fourthtabwidth{0.25\textwidth}
\newlength\halftabwidth
\setlength\halftabwidth{0.5\textwidth}
\newcommand{\choice}[4]{\settowidth\widthcha{AM.#1}\setlength{\widthch}{\widthcha}
    \settowidth\widthchb{BM.#2}
    \ifthenelse{\widthch<\widthchb}{\setlength{\widthch}{\widthchb}}{}
    \settowidth\widthchb{CM.#3}
    \ifthenelse{\widthch<\widthchb}{\setlength{\widthch}{\widthchb}}{}
    \settowidth\widthchb{DM.#4}
    \ifthenelse{\widthch<\widthchb}{\setlength{\widthch}{\widthchb}}{}
    \ifthenelse{\widthch<\fourthtabwidth}{\fourch{#1}{#2}{#3}{#4}}
    {\ifthenelse{\widthch<\halftabwidth\and\widthch>\fourthtabwidth}{\twoch{#1}{#2}{#3}{#4}}
        {\onech{#1}{#2}{#3}{#4}}}}
%choice
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
morekeywords={self},              % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=single,                         % Any extra options here
showstringspaces=false
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
%%%%%
% Bash style for highlighting
\newcommand\bashstyle{\lstset{
language=bash,
basicstyle=\ttm\normalsize,
tabsize=4,
morekeywords={self, head, tail, uniq, sort, grep, cat, cut, echo, wc, cp, rm, mkdir, cd, nano, man, ls, history, bash, rmdir, find, plink, bcftools, bedtools, octopus},              % Add keywords here
keywordstyle=\color{deepblue}\normalsize\bfseries,
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=single,                         % Any extra options here
showstringspaces=false
}}


% Python environment
\lstnewenvironment{bash}[1][]
{
\bashstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\bashexternal[2][]{{
\bashstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\bashinline[1]{{\bashstyle\lstinline!#1!}}
%%%%%
\newcommand{\dollar}{\mbox{\textdollar}}
\newcommand{\dps}[1]{\ensuremath{\displaystyle{#1}}}
\newcommand\ffrac[2]{\ensuremath{\dfrac{\;#1\;}{\;#2\;}}}
\newcommand{\comma}{\, \; \;\mathclap{\text{，}}} %用于mathmode中的逗号
\newcommand{\semicolon}{\, \; \;\mathclap{\text{；}}} %用于mathmode中的分号
\newcommand{\pl}{\phantom{l}} %用来占位
\newcommand{\un}{\ding{172}}
\newcommand{\deux}{\ding{173}}
\newcommand{\trois}{\ding{174}}
\newcommand{\quatre}{\ding{175}}
\newcommand{\et}{&}
\newcommand{\f}{^2}
\newcommand{\xz}{(\qquad)}
\newcommand{\tk}{\underline{\qquad\qquad}}
%高等数学：
\renewcommand{\d}{\,\mathrm{d}}
\newcommand{\dt}{\,\mathrm{d}t}
\newcommand{\dr}{\,\mathrm{d}r}
\newcommand{\du}{\,\mathrm{d}u}
\newcommand{\dv}{\,\mathrm{d}v}
\newcommand{\dx}{\,\mathrm{d}x}
\newcommand{\dy}{\,\mathrm{d}y}
\newcommand{\dz}{\,\mathrm{d}z}
\newcommand{\df}{\,\mathrm{d}f}
\newcommand{\bigmid}{\, \bigg | \,} %用于集合中有分数的情况
\newcommand\matharr{\tikz[baseline=-0.4ex]\draw[-stealth] (0,0) -- + (3mm,0);} %用于下标中的右箭头
\newcommand\textarr{\; \tikz[baseline=-0.55ex]\draw[-stealth] (0,0) -- + (4mm,0);} %用于文本中的右箭头，注意用占位符调整前后间距
\newcommand{\limite}[2]{\ensuremath{\lim\limits_{#1\matharr #2}}} %#1趋向于#2
\newcommand{\dlimite}[4]{\ensuremath{\displaystyle{\lim_{\substack{ \phantom{l}#1\matharr #2\phantom{l} \\ #3\matharr #4}}}}} %#重极限：1趋向于#2，#3趋向于#4，phantom{l}用来占位
\newcommand{\neighbr}{\ensuremath{\mathring{U}(x_0\comma \delta)}} %去心邻域U
\newcommand{\neighbor}{\ensuremath{U(x_0\comma \delta)}} %邻域U
\newcommand{\tikzrm}[1]{
	\fill[white] #1 circle(1.5pt);
	\draw #1 circle(1.5pt);
}
\newcommand{\derivee}[4]{
	\ffrac{\,\mathrm{d}^{#1}#2}{\,\mathrm{d}#3^{#4}}
}
\newcommand{\intscript}[2]{\biggl.\biggr|_{\, #2}^{\, #1}} %求出原函数以后代入的积分上下限
\newcommand{\concept}[1]{\textcolor{magenta}{#1}}
\newcommand{\highlight}[1]{\textcolor{red}{#1}}
\renewcommand{\emph}[1]{\textcolor{blue}{#1}}
\newcommand{\dint}[2]{\ensuremath{\displaystyle{\int_{#2}^{#1}}}}
\newcommand{\diint}[4]{\ensuremath{\displaystyle{\int_{#2}^{#1}\int_{#4}^{#3}}}}
\newcommand{\bint}{\mathlarger{\int}} %用于将幂次上的积分号放大
\newcommand{\exiint}{\ensuremath{\!\!\!}} %用于缩短累次积分中积分号的距离
\newcommand{\fxy}{\ensuremath{f(x\comma y)}}
\newcommand{\xoyo}{\ensuremath{(x_0\comma y_0)}}
\newcommand{\series}{\ensuremath{\dps{\Xsum_{n=1}^\infty}}} %级数
\newcommand{\serieso}{\ensuremath{\dps{\Xsum_{n=0}^\infty}}} %0开始的级数
%线性代数：
\newcommand{\pA}{\ensuremath{\pmb{A}}}
\newcommand{\pB}{\ensuremath{\pmb{B}}}
\newcommand{\pC}{\ensuremath{\pmb{C}}}
\newcommand{\pO}{\ensuremath{\pmb{O}}}
\newcommand{\pP}{\ensuremath{\pmb{P}}}
\newcommand{\pQ}{\ensuremath{\pmb{Q}}}
\newcommand{\pE}{\ensuremath{\pmb{E}}}
\newcommand{\px}{\ensuremath{\pmb{x}}}
\newcommand{\pX}{\ensuremath{\pmb{X}}}
\newcommand{\pR}{\ensuremath{\pmb{R}}}
\newcommand{\pZ}{\ensuremath{\pmb{Z}}}
\newcommand{\pal}{\ensuremath{\pmb{\alpha}}}
\newcommand{\pbe}{\ensuremath{\pmb{\beta}}}
\newcommand{\pxi}{\ensuremath{\pmb{\xi}}}
\newcommand{\pet}{\ensuremath{\pmb{\eta}}}
\renewcommand{\t}{\ensuremath{^\mathrm{T}}}
\newcommand\laarr{\qquad\tikz\draw[-stealth] (0,0) -- + (7mm,0);\qquad} %用于矩阵中的初等变换
\newcommand{\laarrt}[1]{\qquad\tikz\draw[-stealth] (0,0) -- (4mm,0) node[above]{#1}--+ (4mm,0);\qquad} %初等变换上带字
%概率：
\newcommand{\XY}{\ensuremath{(X\comma Y)}}
\newcommand{\Cov}{\ensuremath{\mathrm{Cov}}}
\newcommand{\cip}{\tikz[baseline=-0.55ex]\draw[-stealth] (0,0) -- (2mm,0) node[above]{$\;\;P$}--+ (4mm,0);\;} %依概率收敛
\newcommand{\seriesn}{\ensuremath{\dps{\Xsum_{i=1}^n}}} %1开始到n的连续求和
\begin{document}
%\kaishu
\begin{center}
\Large{Miscellaneous Genomics Notes}
\end{center}
\begin{itemize}
\item Post-imputation information measure\footnote{\url{http://www.nature.com/articles/nrg2796}.}. Let $G_{ij}\in \{0,1,2\}$ denote the genotype of the $i$th individual at the $j$th SNP in a study cohort of $N$ samples. Let $p_{ijk} = P(G_{ij}=k|H,G)$ be the probability (obtained) from imputation that the genotype at the $j$th SNP of the $i$th individual is $k$. Define the \textcolor{magenta}{expected allele dosage} for the genotype at the $j$th SNP of the $i$th individual be
$$
e_{ij} = p_{ij1} + 2p_{ij2}
$$
Note that this equation may define $2e_{ij}$ elsewhere. Let $f_{ij} = p_{ij1} + 4p_{ij2}$, $\theta_j$ denote the unknown population allele frequency of the $j$th SNP with estimate
$$
\hat{\theta} = \dfrac{1}{2N}\Xsum_{i=1}^Ne_{ij}
$$
and $X=\Xsum_{i=1}^NG_{ij}$.
\begin{itemize}
	\item The \textcolor{magenta}{MACH $\hat{r}^2$} is the ratio of the empirically observed variance of the allele dosage to the expected binomial variance at Hardy-Weinberg equilibrium. At the $j$th SNP this is defined as 
	$$
		\hat{r}_j\f = \begin{cases}
			\dfrac{\dfrac{1}{N}\Xsum_{i=1}^N e_{ij}\f - \dfrac{1}{N\f}\left(\Xsum_{i=1}^Ne_{ij}\right)\f}{2\hat{\theta}(1-\hat{\theta})} \et \hat{\theta}\in (0,1)\\
			1 \et \hat{\theta} \in \{0,1\}
		\end{cases}	
	$$
	\item The \textcolor{magenta}{BEAGLE allelic $R\f$} is derived by approximating the $R\f$ between the best guess genotype (the most likely imputed genotype in the $i$th individual at the $j$th SNP, denoted by $z_{ij}$) and the allele dosage as an approximation of the true genotype in the case where the genotype is unknown. At the $j$th SNP this is defined as 
	$$
		R_j\f = \dfrac{\left[\Xsum_iz_{ij}e_{ij}-\dfrac{1}{N}\left(\Xsum_iz_{ij}\Xsum_ie_{ij}\right)\right]\f}{\left[\Xsum_if_{ij}-\dfrac{1}{N}\left(\Xsum_ie_{ij}\right)\f\right]\left[\Xsum_iz_{ij}\f-\dfrac{1}{N}\left(\Xsum_iz_{ij}\right)\f\right]}
	$$
	\item The \textcolor{magenta}{IMPUTE info measure} is based on measuring the relative statistical information about the population allele frequency, $\theta_j$, given by
	$$
		I_A = \begin{cases}
			1 - \dfrac{\Xsum_{i=1}^N(f_{ij} - e_{ij}\f)}{2N(\hat{\theta}(1-\hat{\theta}))} \et \hat{\theta}\in(0,1) \\ 
			1 \et \hat{\theta}\in\{0,1\}
		\end{cases}	
	$$
\end{itemize}
\item The \textcolor{magenta}{SNPTEST info measure} is similar to the IMPUTE info measure when assuming an additive model (but not dominant model) and thus omitted here.
\end{itemize}
The MACH, BEAGLE and IMPUTE measures seem to be highly correlated with BEAGLE $R\f$ systemically reporting lower values and undefined at $3\%$ of the SNPs and MACH $r\f$ often exceeds 1. 
\newpage
\begin{center}
\Large{Algorithms and Technicals}
\end{center}
\begin{itemize}
\item The \concept{Metropolis-Hastings Algorithm}:
\begin{itemize}
	\item Suppose we want to sample a target distribution where we don't know its normalising constant, which we denote as $p(\theta)$, but we have a known distribution $g(\theta)$ such that it satisfies $p(\theta)\propto g(\theta)$. The Metropolis-Hastings algorithm is as follows:
\begin{enumerate}
	\item Select initial value $\theta_0$.
	\item For $i=1,\cdots,m$, repeat:
	\begin{enumerate}
		\item Draw candidate $\theta^*\sim q(\theta^*\mid\theta_{i-1})$.
		\item Calculate $\alpha$ with
		$$
			\alpha = \dfrac{g(\theta^*)/q(\theta^*\mid\theta_{i-1})}{g(\theta_{i-1})/q(\theta_{i-1}\mid\theta^*)} = \dfrac{g(\theta^*)q(\theta_{i-1}\mid \theta^*)}{g(\theta_{i-1})q(\theta^*\mid\theta_{i-1})}
		$$
		\item Determine whether to retain the sample $\theta^*$ based on $\alpha$:
		\begin{itemize}
			\item If \emph{$\alpha\geq 1$}, \highlight{accept} $\theta^*$ and set $\theta_i\leftarrow\theta^*$;
			\item If \emph{$0<\alpha<1$}:
			\begin{itemize}
				\item \highlight{Accept} $\theta^*$ and set $\theta_i\leftarrow\theta^*$ with probability $\alpha$;
				\item \highlight{Reject} $\theta^*$ and set $\theta_i\leftarrow\theta_{i-1}$ with probability $1-\alpha$.
			\end{itemize}
		\end{itemize}
\end{enumerate}		
\end{enumerate}
	\item This algorithm is a valid Markov chain. One can pick $q$ such that:
	\begin{itemize}
		\item $q$ does not depend on the previous draw $\theta_{i-1}$, in this case we need to have $q$ is similar to $p$.
		\item $q$ depends on the previous draw, where we have a \concept{random walk Metropolis-Hastings}.
	\end{itemize}
	\item If we $q$ is normal, we can increase its standard deviation for decreasing acceptance rate. Targeted acceptance rate can be 23\%$-50\%$.
\end{itemize}
\item \concept{Gibbs sampling} is to sample from an unknown distribution if multiple parameters are unknown. When we sample one of the parameters, simply treat the other as a known constant (due to the chain rule of conditional probability).
\begin{itemize}
	\item Let's assume the unknown distribution is $p(\theta,\varphi\mid y)$ but we have $g(\theta,\varphi)$ that satisfies $p(\theta,\varphi\mid y)\propto g(\theta,\varphi)$. The Gibbs sampler is as follows:
\begin{enumerate}
	\item Select initial value $\theta_0$ and $\varphi_0$.
	\item For $i=1,\cdots,m$, repeat the following as one Gibbs cycle:
	\begin{enumerate}
		\item Using $\varphi_{i-1}$ to draw $\theta_i\sim p(\theta\mid\varphi_{i-1}, y)$.
		\item Using $\theta_{i}$ to draw $\varphi_i\sim p(\varphi\mid\theta_{i}, y)$.
		\item Update $(\theta_i,\varphi_i)$ according to the Metropolis-Hastings algorithm.
	\end{enumerate}		
\end{enumerate}
\end{itemize}
\item Autocorrelation:
\begin{itemize}
	\item Autocorrelation is an important structure to inspect in order to determine the validity of a MCMC simulation. If autocorrelation is high for large number of lags (one can inspect the autocorrelation plot), it probably indicates that the simulation hasn't converged to a stationary distribution, and we need to increase the number of samples drawn from the chain or modify model hyperparameters.
	\item Autocorrelation is also important in calculating the effective sample size of our MCMC. The \concept{effective sample size} is how many independent samples from the stationary distribution you would have to draw to have equivalent information in our MCMC. It is essentially the sample size we choose for our Monte Carlo estimation.
	\item Broaderly speaking, an effective sample size is often concerned in two scenarios, that is, when our data is autocorrelated or weighted. An intuitive example would be if our $X_1=\cdots=X_n$ and thus perfectly autocorrelated, we basically just have one sample; or if our samples are weighted such that $w_1=1$ and $w_2=\cdots=w_n=0$, the effective sample size is 1 as well. Mathematically, it is defined as the number of i.i.d. samples required to achieve the same level of variance.
	\item If one only cares the mean of the posterior, an effective sample size of the scale 100 to 1000 is good enough; if one seeks to create a confidence interval, then several thousands of effective samples are required.
\end{itemize}
\end{itemize}
\end{document}
