\documentclass[UTF8]{book}
%\usepackage{ctex}
\usepackage{amsmath}
\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{makeidx}
\usepackage{enumitem}
\usepackage{rotating} 
\usepackage{yhmath}
\usepackage{textcomp,booktabs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{gensymb}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage[all,pdf]{xy}
\usepackage{exscale}
\usepackage{blindtext}
\usepackage{hyperref}
\hypersetup{
colorlinks=true,
linkcolor=black
}
\usepackage{nameref}
\usepackage{relsize}
\usepackage{titlesec}
\usepackage{ifthen}
\usepackage{array}
\usepackage[flushleft]{threeparttable}
\usepackage{diagbox}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{ulem}
\usepackage{xcolor}
\usepackage{color}
\usepackage{mathptmx}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, left=2.54cm,right=2.54cm,top=2.54cm,bottom=2.54cm}
%\geometry{b5paper, left=1.6cm,right=2cm,top=2cm,bottom=2cm}
\usepackage{mathrsfs}
\usepackage{tikz,tkz-euclide}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{shapes,arrows}
\usepackage{esvect}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % clear all fields 
\cfoot{}
\fancyhead[LE,RO]{\thepage} 
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\linespread{1.4}
\date{}
\graphicspath{ {Graphs} }
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  backgroundcolor=\color{gray!20!white}
}
\definecolor{codegray}{gray}{0.8}
\newcommand{\code}[1]{\colorbox{codegray}{\texttt{#1}}}
%通用：
\newcounter{mylabelcounter}
\makeatletter
\newcommand{\labeltext}[2]{%
#1\refstepcounter{mylabelcounter}%
\immediate\write\@auxout{%
  \string\newlabel{#2}{{1}{\thepage}{{\unexpanded{#1}}}{mylabelcounter.\number\value{mylabelcounter}}{}}%
}%
}
%Xsum
\DeclareFontFamily{U} {cmex}{}
\DeclareFontShape{U}{cmex}{m}{n}{
  <-6> cmex5
  <6-7> cmex6
  <7-8> cmex7
  <8-9> cmex8
  <9-10> cmex9
  <10-12> cmex10
  <12-> cmex12}{}
\DeclareSymbolFont{Xcmex} {U} {cmex}{m}{n}
\DeclareMathSymbol{\Xdsum}{\mathop}{Xcmex}{88}
\DeclareMathSymbol{\Xtsum}{\mathop}{Xcmex}{80}
\DeclareMathOperator*{\Xsum}{\mathchoice{\Xdsum}{\Xtsum}{\Xtsum}{\Xtsum}}
%Xsum
%choice
\newcommand{\fourch}[4]{%~\hfill(\qquad)\\
\begin{tabular}{*{4}{@{}p{0.25\textwidth}}}(A)~#1 & (B)~#2 & (C)~#3 & (D)~#4\end{tabular}}
\newcommand{\twoch}[4]{%~\hfill(\qquad)\\
\begin{tabular}{*{2}{@{}p{0.5\textwidth}}}(A)~#1 & (B)~#2\end{tabular}\\\begin{tabular}{*{2}{@{}p{0.5\textwidth}}}(C)~#3 & (D)~#4\end{tabular}}
\newcommand{\onech}[4]{%~\hfill(\qquad)\\
(A)~#1 \\ (B)~#2 \\ (C)~#3 \\ (D)~#4}
 
\newlength\widthcha
\newlength\widthchb
\newlength\widthchc
\newlength\widthchd
\newlength\widthch
\newlength\tabmaxwidth
\setlength\tabmaxwidth{1\textwidth}
\newlength\fourthtabwidth
\setlength\fourthtabwidth{0.25\textwidth}
\newlength\halftabwidth
\setlength\halftabwidth{0.5\textwidth}
\newcommand{\choice}[4]{\settowidth\widthcha{AM.#1}\setlength{\widthch}{\widthcha}
    \settowidth\widthchb{BM.#2}
    \ifthenelse{\widthch<\widthchb}{\setlength{\widthch}{\widthchb}}{}
    \settowidth\widthchb{CM.#3}
    \ifthenelse{\widthch<\widthchb}{\setlength{\widthch}{\widthchb}}{}
    \settowidth\widthchb{DM.#4}
    \ifthenelse{\widthch<\widthchb}{\setlength{\widthch}{\widthchb}}{}
    \ifthenelse{\widthch<\fourthtabwidth}{\fourch{#1}{#2}{#3}{#4}}
    {\ifthenelse{\widthch<\halftabwidth\and\widthch>\fourthtabwidth}{\twoch{#1}{#2}{#3}{#4}}
        {\onech{#1}{#2}{#3}{#4}}}}
%choice
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
morekeywords={self},              % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=single,                         % Any extra options here
showstringspaces=false
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
%%%%%
% Bash style for highlighting
\newcommand\bashstyle{\lstset{
language=bash,
basicstyle=\ttm\normalsize,
tabsize=4,
morekeywords={self, head, tail, uniq, sort, grep, cat, cut, echo, wc, cp, rm, mkdir, cd, nano, man, ls, history, bash, rmdir, find, plink, bcftools, bedtools, octopus, jellyfish, sacct, snakemake, column, df, sbatch, squeue, scancel, sstat, sprio, srun, tmux, sacct},              % Add keywords here
keywordstyle=\color{deepblue}\normalsize\bfseries,
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=single,                         % Any extra options here
showstringspaces=false
}}


% Python environment
\lstnewenvironment{bash}[1][]
{
\bashstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\bashexternal[2][]{{
\bashstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\bashinline[1]{{\bashstyle\lstinline!#1!}}
%%%%%
\newcommand{\dollar}{\mbox{\textdollar}}
\newcommand{\perpp}{\ensuremath{\perp\!\!\!\!\!\perp}}
\newcommand{\dps}[1]{\ensuremath{\displaystyle{#1}}}
\newcommand\ffrac[2]{\ensuremath{\dfrac{\;#1\;}{\;#2\;}}}
\newcommand{\comma}{\, \; \;\mathclap{\text{，}}} %用于mathmode中的逗号
\newcommand{\semicolon}{\, \; \;\mathclap{\text{；}}} %用于mathmode中的分号
\newcommand{\pl}{\phantom{l}} %用来占位
\newcommand{\un}{\ding{172}}
\newcommand{\deux}{\ding{173}}
\newcommand{\trois}{\ding{174}}
\newcommand{\quatre}{\ding{175}}
\newcommand{\et}{&}
\newcommand{\f}{^2}
\newcommand{\xz}{(\qquad)}
\newcommand{\tk}{\underline{\qquad\qquad}}
%高等数学：
\renewcommand{\d}{\,\mathrm{d}}
\newcommand{\dt}{\,\mathrm{d}t}
\newcommand{\dr}{\,\mathrm{d}r}
\newcommand{\du}{\,\mathrm{d}u}
\newcommand{\dv}{\,\mathrm{d}v}
\newcommand{\dx}{\,\mathrm{d}x}
\newcommand{\dy}{\,\mathrm{d}y}
\newcommand{\dz}{\,\mathrm{d}z}
\newcommand{\df}{\,\mathrm{d}f}
\newcommand{\bigmid}{\, \bigg | \,} %用于集合中有分数的情况
\newcommand\matharr{\tikz[baseline=-0.4ex]\draw[-stealth] (0,0) -- + (3mm,0);} %用于下标中的右箭头
\newcommand\textarr{\; \tikz[baseline=-0.55ex]\draw[-stealth] (0,0) -- + (4mm,0);} %用于文本中的右箭头，注意用占位符调整前后间距
\newcommand{\limite}[2]{\ensuremath{\lim\limits_{#1\matharr #2}}} %#1趋向于#2
\newcommand{\dlimite}[4]{\ensuremath{\displaystyle{\lim_{\substack{ \phantom{l}#1\matharr #2\phantom{l} \\ #3\matharr #4}}}}} %#重极限：1趋向于#2，#3趋向于#4，phantom{l}用来占位
\newcommand{\neighbr}{\ensuremath{\mathring{U}(x_0\comma \delta)}} %去心邻域U
\newcommand{\neighbor}{\ensuremath{U(x_0\comma \delta)}} %邻域U
\newcommand{\tikzrm}[1]{
	\fill[white] #1 circle(1.5pt);
	\draw #1 circle(1.5pt);
}
\newcommand{\derivee}[4]{
	\ffrac{\,\mathrm{d}^{#1}#2}{\,\mathrm{d}#3^{#4}}
}
\newcommand{\intscript}[2]{\biggl.\biggr|_{\, #2}^{\, #1}} %求出原函数以后代入的积分上下限
\newcommand{\concept}[1]{\textcolor{magenta}{#1}}
\renewcommand{\emph}[1]{\textcolor{blue}{#1}}
\newcommand{\dint}[2]{\ensuremath{\displaystyle{\int_{#2}^{#1}}}}
\newcommand{\diint}[4]{\ensuremath{\displaystyle{\int_{#2}^{#1}\int_{#4}^{#3}}}}
\newcommand{\bint}{\mathlarger{\int}} %用于将幂次上的积分号放大
\newcommand{\exiint}{\ensuremath{\!\!\!}} %用于缩短累次积分中积分号的距离
\newcommand{\fxy}{\ensuremath{f(x\comma y)}}
\newcommand{\xoyo}{\ensuremath{(x_0\comma y_0)}}
\newcommand{\series}{\ensuremath{\dps{\Xsum_{n=1}^\infty}}} %级数
\newcommand{\serieso}{\ensuremath{\dps{\Xsum_{n=0}^\infty}}} %0开始的级数
%线性代数：
\newcommand{\pA}{\ensuremath{\pmb{A}}}
\newcommand{\pB}{\ensuremath{\pmb{B}}}
\newcommand{\pC}{\ensuremath{\pmb{C}}}
\newcommand{\pO}{\ensuremath{\pmb{O}}}
\newcommand{\pP}{\ensuremath{\pmb{P}}}
\newcommand{\pQ}{\ensuremath{\pmb{Q}}}
\newcommand{\pE}{\ensuremath{\pmb{E}}}
\newcommand{\px}{\ensuremath{\pmb{x}}}
\newcommand{\pX}{\ensuremath{\pmb{X}}}
\newcommand{\pR}{\ensuremath{\pmb{R}}}
\newcommand{\pZ}{\ensuremath{\pmb{Z}}}
\newcommand{\pal}{\ensuremath{\pmb{\alpha}}}
\newcommand{\pbe}{\ensuremath{\pmb{\beta}}}
\newcommand{\pxi}{\ensuremath{\pmb{\xi}}}
\newcommand{\pet}{\ensuremath{\pmb{\eta}}}
\renewcommand{\t}{\ensuremath{^\mathrm{T}}}
\newcommand\laarr{\qquad\tikz\draw[-stealth] (0,0) -- + (7mm,0);\qquad} %用于矩阵中的初等变换
\newcommand{\laarrt}[1]{\qquad\tikz\draw[-stealth] (0,0) -- (4mm,0) node[above]{#1}--+ (4mm,0);\qquad} %初等变换上带字
%概率：
\newcommand{\XY}{\ensuremath{(X\comma Y)}}
\newcommand{\Cov}{\ensuremath{\mathrm{Cov}}}
\newcommand{\cip}{\tikz[baseline=-0.55ex]\draw[-stealth] (0,0) -- (2mm,0) node[above]{$\;\;P$}--+ (4mm,0);\;} %依概率收敛
\newcommand{\seriesn}{\ensuremath{\dps{\Xsum_{i=1}^n}}} %1开始到n的连续求和
\begin{document}
%\kaishu
\begin{center}
\Large{2023-2024 Graphical Model Notes}
\end{center}
\large{\textbf{Chapter 3\quad Exponential Families and Contingency Tables}}
\begin{itemize}
\item Denote $X_V \equiv (X_v: v\in V)$, indexed by $V=\{1,\cdots,p\}$. Each $X_v$ takes values in the set $\mathcal{X}_v$. For a subset $A\subseteq V$, we write $X_A$ to denote $(X_v: v\in A)$.
\item Let $p(\cdot ;\theta)$ be a collection of probability densities over $\mathcal{X}$ indexed by $\theta \in \Theta$. We say that $p$ is an \concept{exponential family} if it can be written as 
\begin{align*}
	p(x;\theta) \et = \exp\left\{\Xsum_i\theta_i\phi_i(x) - A(\theta) - C(x)\right\}\\
	\et = \exp\left\{<\theta,\phi(x)> - A(\theta) - C(x)\right\}
\end{align*}
\begin{itemize}
	\item The family is said to be \concept{regular} if $\Theta$ is a non-empty open set.
	\item The functions $\phi_i$ are the \concept{sufficient statistics}.
	\item The components $\theta_i$ are the \concept{canonical/natural parameters}.
	\item The function $A(\theta)$ is the \concept{cumulant function} such that the distribution normalises:
	$$
		A(\theta) = \log\int\exp\{<\theta , \phi(x)> - C(x)\}\dx	
	$$
	\item The function $Z(\theta) \equiv e^{A(\theta)}$ is the \concept{partition function}.
\end{itemize}
\item We have
$$
\nabla A(\theta) = \mathbb{E}_{\theta}\phi(X) \qquad \nabla\nabla\t A(\theta) = \Cov_{\theta}\phi(X)
$$
$A(\theta)$ and $-\log p(x;\theta)$ are convex in $\theta$, and the map $\mu(\theta):\theta \mapsto \nabla A(\theta)$ is bijective, named the \concept{mean function}. We care about convexity because it does not have multiple local minima, which in turn facilitates computation.
\item Let $X_V = (X_1,\cdots,X_p)\t\in\mathbb{R}^p$ be a random vector. Let $\mu\in\mathbb{R}^p$ and $\Sigma\in\mathbb{R}^{p\times p}$ be a positive definite symmetric matrix. We say that $X_V$ has a \concept{multivariate Gaussian distribution} with $\mu$ and $\Sigma$ if the joint density is 
\begin{align*}
	f(x_V) \et = \dfrac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\exp\left\{-\dfrac{1}{2}(x_V-\mu)\t\Sigma^{-1}(x_V-\mu)\right\}\\
	\et = \dfrac{1}{(2\pi)^{\frac{p}{2}}}\exp\left\{-\dfrac{1}{2}x_V\t Kx_V + \mu\t Kx_V - \dfrac{1}{2}\mu\t K\mu + \dfrac{1}{2}\log |K|\right\} \qquad x_V\in\mathbb{R}^p
\end{align*}
Here, $K\equiv\Sigma^{-1}$ is called the \concept{concentration matrix}. Let 
$$
\phi(x_V) = \left(x_v, -\dfrac{1}{2}x_Vx_V\t\right) \qquad \theta = (K\mu ,K)
$$
we could easily tell that the multivariate Gaussian distribution is an exponential family\footnote{For two matrices $A$ and $B$, we have $<A,B>=\mathrm{tr}(A,B\t)$.}.
\item Let $X_V$ have a multivariate Gaussian distribution with concentration matrix $K=\Sigma^{-1}$, then $X_i\perpp X_j\mid X_{V\setminus\{i,j\}}$ iff $k_{ij} = 0$.
\item Let $X_V^{(i)}=(X_1^{(1)},\cdots,X_p^{(i)})$ be sampled over individuals $i=1,\cdots , n$ and define
$$
n(x_V) \equiv \Xsum_{i=1}^n\mathbbm{1}\{X_1^{(i)}=x_1,\cdots,X_p^{(i)}=x_p\}
$$
the number of individuals who have the response pattern $x_V$. These counts are the sufficient statistics for the multinomial model with log-likelihood
\begin{align*}
	l(p; n)\et = \Xsum_{x_V}n(x_V)\log p(x_V)\\
	\et = \Xsum_{x_V\neq 0_V} n(x_V)\log\dfrac{p(x_V)}{p(0_V)} +n\log p(0_V)
\end{align*}
where $0_V$ is the vector of zeros, $p(x_V)\geq 0$, and $\Xsum_{x_V}p(x_V)=1$. The multinomial distribution is also an exponential family with
\begin{itemize}
	\item Sufficient statistics given by $n(x_V)$.
	\item Canonical parameters given by $\log\dfrac{p(x_V)}{p(0_V)}$.
	\item Convex cumulant function given by
	$$
		-\log p(0_V) = \log\left(1+\Xsum_{x_V\neq 0_V}e^{\theta (x_V)}\right)	
	$$
\end{itemize}
Each possibility $x_V$ is called a \concept{cell} of the table. Given $A\subseteq V$, 
$$
n(x_A)\equiv \Xsum_{x_B}n(x_A,x_B)
$$
where $B=V\setminus A$ is called the \concept{marginal table}.
\item The \concept{log-linear} parameters for $p(x_V)>0$ are defined by the relation
\begin{align*}
	\log p(x_V) \et = \Xsum_{A\subseteq V}\lambda_A(x_A)\\
	\et = \lambda_{\varnothing} + \lambda_1(x_1)+\cdots+\lambda_V(x_V)
\end{align*}
and the identifiability constraint $\lambda_A(x_A) = 0$ whenever $x_a = 1$ for some $a\in A$.
\item Consider a $2\times 2$ contingency table with probabilities $\pi_{ij}$. The log-linear parametrisation has
\begin{alignat*}{2}
\log\pi_{11} \et = \lambda_{\varnothing} \qquad \et\et\log\pi_{21}  = \lambda_{\varnothing} + \lambda_X \\
\log\pi_{12} \et = \lambda_{\varnothing} + \lambda_Y \qquad \et\et\log\pi_{22}  = \lambda_{\varnothing} + \lambda_X + \lambda_Y + \lambda_{XY}
\end{alignat*}
We can deduce that 
$$
\lambda_{XY} = \log\dfrac{\pi_{11}\pi_{22}}{\pi_{21}\pi_{12}}
$$
and $e^{\lambda_{XY}}$ is called the \concept{odds ratio} between X and Y.
\item Let $X_i\sim \mathrm{P}(\mu_i)$ independently, and let $N=\Xsum_{i=1}^k X_i$. Then,
\begin{align*}
	N \et \sim \mathrm{P}(\Xsum_i\mu_i)\\
	(X_1,\cdots,X_k)\t\mid N = n\et \sim \mathrm{Multinom}(n, (\pi_1,\cdots,\pi_k)\t)
\end{align*}
where $\pi_i = \dfrac{\mu_i}{\Xsum_j\mu_j}$.
\item Let $p>0$ be a discrete distribution on $X_V$ with associated log-linear parameters $\lambda_C, C\subseteq V$. The conditional independence $X_a\perpp X_b\mid X_{V\setminus\{a,b\}}$ holds if and only if $\lambda_C = 0$ for all $\{a,b\}\subseteq C\subseteq V$.
\end{itemize}
\large{\textbf{Chapter 4\quad Undirected Graphical Models}}
\begin{itemize}
\item Let $V$ be a finite set. An \concept{undirected graph} $\mathcal{G}$ is a pair $(V,E)$ where
\begin{itemize}
	\item $V$ are the \concept{vertices/nodes}.
	\item $E\subseteq\{\{i,j\}: i,j\in V, i\neq j\}$ is a set of unordered distinct pairs of $V$ called \concept{edges}.
\end{itemize}
We represent graphs by drawing the vertices and then joining pairs of vertices by a line if there is an edge between them.
\begin{itemize}
	\item We write $i\sim j$ if $\{i,j\}\in E$, and say they are \concept{adjacent} in the graph. The vertices adjacent to $i$ are called the \concept{neighbours} of $i$, and the set of neighbours is often called the \concept{boundary} of $i$ and denoted by $\mathrm{bd}_{\mathcal{G}}(i)$.
	\item A \concept{path} in a graph is a sequence of adjacent vertices without repetition. The \concept{length} of a path is the number of edges in it.
	\item Given a subset of vertices $W\subseteq V$, we define the \concept{induced subgraph} $\mathcal{G}_W$ of $\mathcal{G}$ to be the graph with vertices $W$, and all edges from $\mathcal{G}$ whose endpoints are contained in $W$.
	\item We say $C\subseteq V$ is \concept{complete} if $i\sim j$ for every $i,j\in C$. A maximal\footnote{\emph{Maximal} means if one is to add another vertex into the graph, the graph wil no longer be complete. However, graphs in the cliques do not necessarily need to have the same number of vertices.} complete set is called a \concept{clique}. The set of cliques in a graph is denoted by $\mathcal{C}(\mathcal{G})$.
\end{itemize}
\item Let $A,B,S\subseteq V$. We say that $A$ and $B$ are \concept{separated} by $S$ in $\mathcal{G}$ ($A\perp_s B\mid S[\mathcal{G}]$) if every path from any $a\in A$ to any $b\in B$ contains at least one vertex in $S$. $A$ and $B$ are separated by $S$ (where $S\cap A=S\cap B=\varnothing$) iff $A$ and $B$ are separated by $\varnothing$ in $\mathcal{G}_{V\setminus S}$.
\item Let $\mathcal{G}$ be a graph with vertices $V$, and let $p$ be a probability distribution over the random variables $X_V$. We say that $p$ satisfies the \concept{pairwise Markov property} for $\mathcal{G}$ if
$$
i\not\sim j \in\mathcal{G} \implies X_i\perpp X_j\mid X_{V\setminus\{i,j\}}[p]
$$
We say that $p$ satisfies the \concept{global Markov property} for $\mathcal{G}$ if for any disjoint sets $A,B,S$
$$
A\perp_s B\mid S \subseteq\mathcal{G} \implies X_A\perpp X_B\mid X_S[p]
$$
\item A distribution $p$ is said to \concept{factorises} according to graph $\mathcal{G}$ if
$$
p(x_V) = \prod_{C\in\mathcal{C}(\mathcal{G})}\psi_C(x_C)
$$
The functions $\psi_C:\mathbb{R}^{|c|}\rightarrow \mathbb{R}$ are called \concept{potentials}.
\item If $p(x_V)$ factorises according to $\mathcal{G}$, then $p$ is globally Markov with respect to $\mathcal{G}$.
\item (\concept{Hammersley-Clifford Theorem}). If $p(x_V)>0$ obeys the pariwise Markove property with respect to $\mathcal{G}$, then $p$ factorises according to $\mathcal{G}$.
\begin{itemize}
	\item The followings always hold:
	\begin{center}
		factorisation $\Rightarrow$ global Markov $\Rightarrow$ pairwise Markov
	\end{center}
	The following holds if $p$ is strictly positive:
	\begin{center}
		pariwise Markov $\Rightarrow$ factorisation.
	\end{center}
\end{itemize}
\item Given a graph $\mathcal{G}$ with vertices $V=A\cup B\cup S$ where $A,B,S$ are disjoint sets. We say that $(A,S,B)$ consititutes a \concept{decomposition} of $\mathcal{G}$ if:
\begin{itemize}
	\item $\mathcal{G}_S$ is complete;
	\item $A$ and $B$ are separated by $S$ in $\mathcal{G}$
\end{itemize}
If $A$ and $B$ are both non-empty, we say the decomposition is \concept{proper}. If not, we say the decomposition is a \concept{prime}.
\item A graph is decomposable if either it is complete or there is a proper decomposition $(A,S,B)$ and $\mathcal{G}_{A\cup S}$, $\mathcal{G}_{B\cup S}$ are decomposable.
\item Let $C_1,C_2,\cdots,C_k$ be a collection of subsets. We say that the sequence satisfies the \concept{running intersection property (RIP)} if $\forall j\geq 2$,
$$
C_j\cap\bigcup_{i=1}^{j-1}C_i = C_j\cap C_{\sigma(j)} \qquad \sigma(j)<j
$$
\item If $C_1,\cdots,C_k$ satisfy the running intersection property, then there is a graph whose cliques are $\mathcal{C}=\{C_1,\cdots,C_k\}$.
\item Let $\mathcal{G}$ be an undirected graph. A \concept{cycle} is a sequence of vertices $<v_1,\cdots,v_k>$ for $k\geq 3$ such that there is a path $v_1 - \cdots - v_k$ and an edge $v_k - v_1$. A \concept{chord} on a cycle is any edge between two vertices not adjacent on the cycle. A graph is \concept{chordal} or \concept{triangulated} if whenever there is a cycle of length greater or equal to 4, it contains a chord.
\item Let $\mathcal{G}$ be an undirected graph. The followings are equivalent:
\begin{itemize}
	\item $\mathcal{G}$ is decomposable;
	\item $\mathcal{G}$ is triangulated;
	\item Every minimal separator of $a\not\sim b$ is complete;
	\item The cliques of $\mathcal{G}$ satisfy the running intersection property, starting with $C$.
\end{itemize}
\item A \concept{forest} is a graph that contains no cycles. If a forest is connected we call it a \concept{tree}.
\item Let $\mathcal{G}$ be a decomposable graph, and let $C_1,\cdots,C_k$ be an ordering of the cliques which satisfies RIP. Define the $j-$th \concept{separator set} for $j\geq 2$ as
$$
S_j \equiv C_j\cap \bigcup_{i=1}^{j-1} C_i = C_j\cap C_{\sigma(j)}
$$
by convention $S_1 = \varnothing$.
\item Let $\mathcal{G}$ be a graph with decomposition $(A,S,B)$, and let $p$ be a distribution, then $p$ factorises with respect to $\mathcal{G}$ iff its marginals $p(x_{A\cup S})$ and $p(x_{B\cup S})$ factorise according to $\mathcal{G}_{A\cup S}$ and $\mathcal{G}_{B\cup S}$, and
$$
p(x_V) \cdot p_{x_S} = p(x_{A\cup S})\cdot p(x_{B\cup S})
$$
\item Let $\mathcal{G}$ be a decomposable graph with cliques $C_1, \cdots, C_k$, then $p$ factorises with respect to $\mathcal{G}$ iff
$$
p(x_V) = \prod_{i=1}^k p(x_{C_i\setminus S_i}\mid x_{S_i}) = \prod_{i=1}^k\dfrac{p(x_{C_i})}{p(x_{S_i})}
$$
\item Let $\mathcal{G}$ be an undirected graph, and suppose we have counts $n(x_V)$. Then the MLE $\hat{p}$ under the set of distributions that are Markov to $\mathcal{G}$ is the unique element in which
$$
n\cdot\hat{p}(x_C) = n(x_C)
$$
for each clique $C\in \mathcal{C}(\mathcal{G})$.
\item The \concept{iterative proportional fitting (IPF)/iterative proportional scaling (IPS)} algorithm starts with a discrete distribution that satisfies the Markov property for the graph $\mathcal{G}$ (usually pick uniform distribution), and then iteratively fixes each margin $p(x_C)$ to match the required distribution using the update step:
\begin{align*}
	p^{(t+1)}(x_V) \et = p^{(t)}(x_V)\cdot\dfrac{\hat{p}(x_C)}{p^{(t)}(x_C)} \\
	\et = p^{(t)}(x_{V\setminus C}\mid x_C)\cdot\hat{p}(x_C)
\end{align*}
The algorithm is:
\begin{algorithm}
\caption{IPF algorithm}
\begin{algorithmic}
\Function {IPF}{collection of consistent margins $q(x_{C_i})$ for sets $C_1,\cdots,C_k$}
\State set $p(x_V)$ to uniform distribution;
\While{$\max_i\max_{x_{C_i}}|p(x_{C_i}) - q(x_{C_i})| > \mathrm{tol}$}
\For{$i$ in $1,\cdots,k$}
    \State update $p(x_V)$ to $p(x_{V\setminus C_i}\mid x_{C_i})\cdot q(x_{C_i})$;
\EndFor
\EndWhile
\State \Return {distribution $p$ with margins $p(x_{C_i})=q(x_{C_i})$}
\EndFunction
\end{algorithmic}
\end{algorithm}
The sequence of distributions in IPF converges to MLE $\hat{p}(x_V)$.
\end{itemize}
\large{\textbf{Chapter 5\quad Gaussian Graphical Models}}
\begin{itemize}
\item Throughout this course, we assume $\mu = 0$. Let $X_V\sim N_p(\mu,\Sigma)$, and $A$ be a $q\times p$ matrix of full rank $q$. Then,
$$
AX_V\sim N_q(A\mu , A\Sigma A\t)
$$
In particular, for any $U\subseteq V$, we have $X_U\sim N_q(\mu_U,\Sigma_{UU})$.

The MLEs for multivariate Gaussian distribution are 
$$
\hat{\mu} = \bar{X}_v = \dfrac{1}{n}\Xsum_{i=1}^n X_V^{(i)} \qquad \hat{\Sigma} = W = \dfrac{1}{n}\Xsum_{i=1}^n\left(X_V^{(i)} - \bar{X}_V\right)\f
$$
\item $X_A\perpp X_B$ iff $\Sigma_{AB}=0$. $X\perpp Y$ and $X\perpp Z$ implies $X\perpp Y,Z$ for jointly Gaussian random variables.
\item Let $\mathcal{G}$ be a graph with a decomposition $(A,S,B)$, and $X_V\sim N_m(0, \Sigma)$ where $m = |V|$. Then, $X_V$ satisfies the global Markov property with respect to $\mathcal{G}$ only if
$$
\Sigma^{-1} = \{(\Sigma_{A\cup S, A\cup S})^{-1}\}_{A\cup S, A\cup S} + \{(\Sigma_{B\cup S, B\cup S})^{-1}\}_{B\cup S, B\cup S} - \{(\Sigma_{S,S})^{-1}\}_{S,S}
$$
Applying this result to a decomposable graph repeatedly, we see that $X_V$ is Markov with respect to $\mathcal{G}$ iff
$$
\Sigma^{-1} = \Xsum_{i=1}^k\{(\Sigma_{C_i,C_i})^{-1}\} - \Xsum_{i=2}^k\{(\Sigma_{S_i,S_i})^{-1}\}_{S_i, S_i}
$$
Note this notation: If $M$ is a matrix whose rows and columns are indexed by $A\subseteq V$, we write $\{M\}_{A,A}$ to indicate the matrix indexed by $V$ (i.e. it has $|V|$ rows and columns) whose $A,A-$entries are $M$ and with zeros elsewhere. For example, if $|V|=3$, then
$$
M = \left(\begin{matrix}
a \et b\\
b \et c
\end{matrix}\right) \qquad \{M\}_{12,12} = \left(\begin{matrix}
a \et b \et 0\\
b \et c \et 0\\
0 \et 0 \et 0
\end{matrix}\right)
$$
where 12 is used as an abbreviation for $\{1,2\}$ in the subscript.
\item MLE for a decomposable Gaussian graphical model is the unique $\Sigma$ such that $k_{ij} = 0$ if $i\not\sim j$ and $\sigma_{ij} = w_{ij}$ if $i\sim j$.
\end{itemize}
\large{\textbf{Chapter 6\quad Directed Graphs}}
\begin{itemize}
\item A \concept{directed graph} $\mathcal{G}$ is a pair $(V,D)$ where
\begin{itemize}
	\item $V$ is a set of \concept{vertices}.
	\item $D\subseteq\{(i,j): i,j\in V, i\neq j\}$ is a set of ordered distinct pairs of $V$ called \concept{edges}. If $(i,j)\in D$, we write $i\rightarrow j$.
\end{itemize}
We represent graphs by drawing the vertices and then joining pairs of vertices by a directed line if there is an edge between them.
\begin{itemize}
	\item A \concept{path} in a graph is a sequence of adjacent vertices without repetition. The \concept{length} of a path is the number of edges in it. A path with length 0 is a single vertex. The \concept{path} is directed if all the arrows point away from the start.
	\item A \concept{directed cycle} is a directed path from $i$ to $j \neq i$, together with $j\rightarrow i$.
	\item Graphs that contain no directed cycles are called \concept{acyclic}, or \concept{directed acyclic graphs (DAGs)}.
	\item Note the following concepts:
	$$
		i\rightarrow j \quad \begin{cases}
			i \in \mathrm{pa}_\mathcal{G}(j) \et i \mathrm{\;\, is\;\, a\;\, \concept{parent}\;\, of\;\, } j\\
			j \in \mathrm{ch}_{\mathcal{G}}(i) \et j \mathrm{\;\, is\;\, a\;\, \concept{child}\;\, of\;\, }i
		\end{cases}
	$$
	$$
		a\rightarrow\cdots\rightarrow b \mathrm{\;\, or \;\, a = b} \begin{cases}
		a \in \mathrm{an}_\mathcal{G}(b) \et a \mathrm{\;\, is\;\, an \;\, \concept{ancestor} \;\, of\;\,} b \\
		b \in \mathrm{de}_\mathcal{G}(a) \et b \mathrm{\;\, is\;\, a\;\, \concept{descendant} \;\, of \;\,} a
		\end{cases}
	$$
	If $w\not\in$de$_\mathcal{G}(v)$, then $w$ is a \concept{non-descendant} of $v$:
	$$
		\mathrm{nd}_\mathcal{G}(v) = V\setminus \mathrm{de}_\mathcal{G}(v)	
	$$
	\item If the graph is acyclic, we can find a \concept{topological ordering} (i.e. one in which no vertex comes before any of its parents). Given $x_i$, define
	$$
		\mathrm{pre}(i) = \{x_1,\cdots , x_{i-1}\}	
	$$
\end{itemize}
\item For any multivariate distribution, we can factorise it as
$$
p(x_V) = \prod_{i=1}^m p(x_i\mid x_{i,\cdots ,x_{i-1}})
$$
Let $\mathcal{G}$ be a DAG. We say that $p(x_V)$ \concept{factorises} according to $\mathcal{G}$ if
$$
p(x_V) = \prod_{i=1}^m p(x_i\mid x_{\mathrm{pa}_\mathcal{G}}(i))
$$
Given a topological ordering, we require that
$$
p(x_i\mid x_{1,\cdots ,x_{i-1}}) = p(x_i\mid x_{\mathrm{pa}_\mathcal{G}(i)})
$$
which is to say $X_i\perpp X_{\mathrm{pre}(i)\setminus \mathrm{pa}(i)}\mid X_{\mathrm{pa}(i)}$. Note that the ordering is arbitrary, let $\mathrm{pre}(i) = \mathrm{nd}_\mathcal{G}(i)$ and obtain
$$
	X_i\perpp X_{\mathrm{nd}(i)\setminus \mathrm{pa}(i)}\mid X_{\mathrm{pa}(i)} \qquad \forall i\in V
$$
Distributions that have these independences satisfy the \concept{local Markov property}.
\item A set of vertices is \concept{ancestral} if it contains all of its own ancestors.
\item Let $\mathcal{G}$ be a DAG with an ancestral set $A$. Then, $p(x_V)$ factorises according to $\mathcal{G}$ only if $p(x_A)$ factorises according to $\mathcal{G}_A$.
\item A \concept{$v$-structure/unshielded collider} is a triple $i\rightarrow k\leftarrow j$ where $i\not\sim j$.
\item The \concept{moral graph} for a DAG $\mathcal{G}$ is an undirected graph $\mathcal{G}^m$ such that 
$$
i\sim j \; [\mathcal{G}^m] \Leftrightarrow \begin{cases}
i\sim j \; [\mathcal{G}] \\
i\rightarrow k\leftarrow j \; [\mathcal{G}]
\end{cases}
$$
\item If $p(x_V)$ factorises according to a DAG $\mathcal{G}$, then it also factorises according to $\mathcal{G}^m$.
\item We say that a probability density $p(x_V)$ satisfies the \concept{global Markov property} for a DAG $\mathcal{G}$ if wherever $A\perp_s B\mid C$ in $(\mathcal{G}_{\mathrm{an}(A\cup B\cup C)})^m$ we have $X_A\perpp X_B\mid X_C$ under $p$.
\item Let $\mathcal{G}$ be a DAG and $p(x_V)$ be a probability density. Then, the followings are equivalent:
\begin{itemize}
	\item $p$ factorises according to $\mathcal{G}$;
	\item $p$ is globally Markov with respect to $\mathcal{G}$;
	\item $p$ is locally Markov with respect to $\mathcal{G}$.
\end{itemize}
E.g.: Let $X_V$ be a multinomial random vector with probabilities $p(x_V)$, then the log-likelihood if $p$ factorises according to a DAG $\mathcal{G}$ is 
\begin{align*}
	l(p; n) \et = \Xsum_{x_v\in X_V} n(x_v)\log p(x_v)\\
	\et = \Xsum_{x_v\in X_V}n(x_V)\Xsum_{i=1}^m \log p(x_i\mid x_{\mathrm{pa}(i)})\\
	\et = \Xsum_{i=1}^m\Xsum_{x_v\in X_V} n(x_V)\log p(x_i\mid x_{\mathrm{pa}(i)})\\
	\et = \Xsum_{i=1}^m\Xsum_{x_{i\cup \mathrm{pa}(i)}\in X_{\{i\}\cup\mathrm{pa}(i)}}\log p(x_i\mid x_{\mathrm{pa}(i)})\Xsum_{x_{V\setminus(\{i\}\cup\mathrm{pa}(i))} \in X_{V\setminus(\{i\}\cup\mathrm{pa}(i))}} n(x_V)\\
	\et = \Xsum_{i=1}^m\Xsum_{x_{\{i\}\cup\mathrm{pa}(i)}}\log p(x_i\mid x_{\mathrm{pa}(i)})
\end{align*}
The MLE can then be calculated:
$$
\hat{p}(x_i\mid x_{\mathrm{pa}(i)}) = \dfrac{n(x_i, x_{\mathrm{pa}(i)})}{n(x_{\mathrm{pa}(i)})}
$$
For a Bayesian, one may have parameters $\Theta_i$ for each $p(x_i\mid x_{\mathrm{pa}(i)})$. If we choose independent priors, then
\begin{align*}
	\pi(\theta\mid n) \et \propto \pi(\theta)L(\theta\mid n)\\
	\et = \prod_{i=1}^m \pi(\theta_i)f(\theta_i; n(x_i,x_{\mathrm{pa}(i)}))
\end{align*}
This is to say that
$$
\theta_i\perpp X_{V\setminus (\{i\}\cup\mathrm{pa}(i))},\theta_{V\setminus\{i\}}\mid X_i, X_{\mathrm{pa}(i)}
$$
\item For undirected graphs, missing edge induces an independence, hence all graphs give distinct models. If two graphs $\mathcal{G}$ and $\mathcal{G}'$ induce the same statistical model, we say that they are \concept{Markov equivalent}.
E.g.: These models are Markov equivalent:
\begin{itemize}
	\item $X\rightarrow Z\rightarrow Y$:
	$$
		p(x)p(z\mid x)p(y\mid z) \Leftrightarrow X\perpp Y\mid Z	
	$$
	\item $X\leftarrow Z\leftarrow Y$.
	\item $X\leftarrow Z\rightarrow Y$:
	$$
	    p(z)p(x\mid z)p(y\mid z) \Leftrightarrow X\perpp Y\mid Z
	$$
	\item $X-Z-Y$:
	$$
		p(x,y,z) = \psi_{XZ}(x,z)\cdot\psi_{YZ}(y,z) \Leftrightarrow X\perpp Y\mid Z	
	$$
\end{itemize}
\item Given a DAG $\mathcal{G}$, we define its \concept{skeleton} as the undirected graph $\mathrm{skel}(\mathcal{G})$ with the same nodes/vertices and the same adjacencies as $\mathcal{G}$.
\item Let $\mathcal{G},\mathcal{G}'$ be graphs with different skeletons (DAG or undirected). Then, $\mathcal{G}$ and $\mathcal{G}'$ are not Markov equivalent. 
\item Directed graphs $\mathcal{G}$ and $\mathcal{G}'$ are Markov equivalent iff they have the same skeleton and $v$-structures.
\item An undirected graph is Markov equivalent to a directed graph iff it is decomposable.
\end{itemize}
\large{\textbf{Chapter 7\quad Junction Trees and Message Passing}}
\begin{itemize}
\item A connected, undirected graph without any cycles is called a \concept{tree}, denoted by $\mathcal{T}$. Let $\mathcal{V}$ be vertices contained in the power set of $V$, that is, each vertex of $\mathcal{T}$ is a subset of $V$. We say that $\mathcal{T}$ is a \concept{junction tree} if whenever we have $C_i,C_j\in \mathcal{V}$ with $C_i\cap C_j\neq\varnothing$, there is a (unique) path $\pi$ in $\mathcal{T}$ from $C_i$ to $C_j$ such that for every vertex $C$ on the path, $C_i\cap C_j\subseteq C$.
\item If $\mathcal{T}$ is a junction tree. then its vertices $\mathcal{V}$ can be ordered to satisfy the r.i.p. Conversely, if a collection of sets satisfies the r.i.p., they can be arranged into a junction tree. A tree that does not satisfy r.i.p. is sometimes called a \concept{clique tree}.
\item We will associate each node $C$ in our junction tree with a potential $\psi_C(x_C)\geq 0$, which is a function over the variables in the corresponding set. We say that two potentials $\psi_C,\psi_D$ are \concept{consistent} if 
$$
\Xsum_{x_{C\setminus D}}\psi_C(x_C) = f(x_{C\cap D}) = \Xsum_{x_{D\setminus C}}\psi_D(x_D)
$$
That is, the margins of $\psi_C$ and $\psi_D$ over $C\cap D$ are the same.
\item Let $C_1,\cdots,C_k$ satisfy the r.i.p. with separator sets $S_2,\cdots,S_k$, and let
$$
p(x_V) = \prod_{i=1}^k\dfrac{\psi_{C_i}(x_{C_i})}{\psi_{S_i}(x_{S_i})}
$$
where $S_1=\varnothing$ and $\psi_\varnothing = 1$ by convention. Then, each $\psi_{C_i}(x_{C_i})=p(x_{C_i})$ and $\psi_{S_i}(x_{S_i}) = p(x_{S_i})$ iff each pair of potentials is consistent.
\item If a graph is not decomposable, then we can triangulate it by adding edges.
\item Suppose that two cliques $C$ and $D$ are adjacent in the junction tree, with a separator set $S=C\cap D$. An \concept{update} from $C$ to $D$ consists of replacing $\psi_S$ and $\psi_D$ with the following:
$$
\psi_S'(x_S) = \Xsum_{x_{C\setminus S}}\psi_C(x_C) \qquad \psi_D'(x_D) = \dfrac{\psi_S'(x_S)}{\psi_S(x_S)}\psi_D(x_D)
$$
This operation is also known as \concept{message passing}, with the message $\psi_S'(x_S)$ being passed from $C$ to $D$. We note three important points about this updating step:
\begin{itemize}
	\item After updating, $\psi_C$ and $\psi_S'$ are consistent.
	\item If $\psi_D$ and $\psi_S$ are consistent, then so are $\psi'_D$ and $\psi_S'$.
	\item The product over all clique potentials
	$$
		\dfrac{\prod_{C\in \mathcal{C}}\psi_C(x_C)}{\prod_{S\in \mathcal{S}}\psi_S(x_S)}	
	$$
	is unchanged: the only altered terms are $\psi_D$ and $\psi_S$, and by definition of $\psi_D'$ we have
	$$
		\dfrac{\psi_D'(x_D)}{\psi_S'(x_S)} = \dfrac{\psi_D(x_D)}{\psi_S(x_S)}	
	$$
\end{itemize}
Hence, updating preserves the joint distribution and does not upset margins that are already consistent. The junction tree algorithm is a way of updating all the margins such that, when it is complete, they are all consistent.
\item Let $\mathcal{T}$ be a tree. Given any node $t\in\mathcal{T}$, we can root the tree at $t$, and replace it with a directed graph in which all the edges point away from $t$. The \concept{junction tree algorithm} involves messages being passed from the edge of the junction tree (the leaves) towards a chosen root (the \concept{collection phase}), and then being sent away from that root back down to the leaves (the \concept{distribution} phase). Once these steps are completed, the potentials will all be consistent. This process is also called \concept{brief propagation}.
\begin{algorithm}
\caption{Collect and distribute steps of the junction tree algorithm}
\begin{algorithmic}
\Function {COLLECT}{rooted tree $\mathcal{T}$, potentials $\psi_t$}
\State let $1<\cdots <k$ be a topological ordering of $\mathcal{T}$
\For{$t$ in $k,\cdots,2$}
    \State send message from $\psi_t$ to $\psi_{\sigma(t)}$;
\EndFor
\State \Return {updated potentials $\psi_t$}
\EndFunction
\Function {DISTRIBUTE}{rooted tree $\mathcal{T}$, potentials $\psi_t$}
\State let $1<\cdots <k$ be a topological ordering of $\mathcal{T}$
\For{$t$ in $2,\cdots,k$}
    \State send message from $\psi_{\sigma(t)}$ to $\psi_t$;
\EndFor
\State \Return {updated potentials $\psi_t$}
\EndFunction
\end{algorithmic}
\end{algorithm}
\item Let $\mathcal{T}$ be a junction tree with potentials $\psi_{C_i}(x_{C_i})$. After running the junction tree algorithm, all pairs of potentials will be consistent.
\item In practice, message passing is often done in parallel, and it is not hard to prove that if all potentials update simultaneously, then the potentials will converge to a consistent solution in at most $d$ steps, where $d$ is the width (the length of the longest path) of the tree.
\item E.g.: Suppose we have just two tables, $\psi_{XY}$ and $\psi_{YZ}$ arranged in the juntion tree representing a distribution in which $X\perpp Z\mid Y$. We can initialise by setting
$$
\psi_{XY}(x,y) = p(x\mid y) \qquad \psi_{YZ}(y,z) = p(y, z) \qquad \psi_Y(y) = 1
$$
so that $p(x,y,z) = p(y,z)\cdot p(x\mid y)=\dfrac{\psi_{YZ}\psi_{XY}}{\psi_Y}$. Now, we could pick $YZ$ as the root node of our tree, so the collection step consists of replacing 
$$
\psi_Y'(y) = \Xsum_x\psi_{XY}(x,y) = \Xsum_xp(x\mid y) = 1
$$
so $\psi_Y'$ and $\psi_Y$ are the same. Hence, the collection step leaves $\psi_Y$ and $\psi_{YZ}$ unchanged. The distribution step consists of 
\begin{align*}
	\psi_Y''(y) \et = \Xsum_z\psi_{YZ}(y,z) = \Xsum_zp(y,z) = p(y)\\
	\psi_{XY}'(x,y) \et = \dfrac{\psi_Y''(y)}{\psi_Y(y)}\psi_{XY}(x,y) = \dfrac{p(y)}{1}p(x\mid y) = p(x,y)
\end{align*}
Hence, after performing both steps, each potential is the marginal distribution corresponding to those variables.
\item In junction graphs that are not trees, it is still possible to perform message passing, but convergence is not guranteed. This is known as \concept{loopy belief propagation}, and is a topic of current research.
\item Back to the lung cancer example. Suppose we know a person smokes, then we replace $p(s)$ with $\mathbbm{1}_{\{s=1\}}$. Then, we can run a DISTRIBUTE step to obtian other tables conditional on being a smoker. Suppose we have multiple conditions, it is necessary to DISTRIBUTE once for each condition.
\end{itemize}
\large{\textbf{Chapter 8\quad Causal Inference}}
\begin{itemize}
\item A pair $(\mathcal{G}, p)$ is said to be \concept{causal} if
$$
p(x_{V\setminus A}\mid \mathrm{do}(x_A)) = \prod_{i\in V\setminus A}p(x_i\mid x_{\mathrm{pa}(i)}) \qquad \forall A\subseteq V, x_v\in \mathcal{X}_V
$$
Here, "\emph{do}" represents an intervention to set $X_A = x_A$. If we interwene on $X$, we delete all incoming edges in graph $\mathcal{G}$. Note that it is neither a conditional nor an ordinary marginal distribution.
\item Example: Let $Z\rightarrow X\rightarrow Y, Z\rightarrow Y$ be the DAG. We have
$$
p(y\mid\mathrm{do}(x)) = \Xsum_{z\in\mathcal{Z}} p(z)p(y\mid x,z)
$$
Note that
\begin{align*}
	p(y\mid\mathrm{do}(x)) \et = \Xsum_z p(z)p(y\mid z,x)\\
	\et \neq \Xsum_z p(z\mid x)p(y\mid z,x)\\
	\et = p(y\mid x)
\end{align*}
This formula is called an \concept{adjustment} or \concept{standardisation} or the \concept{$g$-formula}.
\item Later we usually denote $T$ to be \emph{treatment} or \emph{intervention} and $Y$ to be \emph{outcome}. The following holds:
$$
p(y\mid\mathrm{do}(t)) = \Xsum_{x_{\mathrm{pa}(t)}}p(x_{\mathrm{pa}(t)})p(y\mid t, x_{\mathrm{pa}(t)})
$$
\item Let $\mathcal{G}$ be a DAG and $\pi$ a path in $\mathcal{G}$. An \concept{internal vertex} is any that does not begin or end $\pi$. Such a vertex $c$ is a \concept{collider} if both edges on $\pi$ contained and point to $c$ (namely, $\rightarrow c\leftarrow$). Otherwise it is a \concept{non-collider}.
\item A path form $a$ to $b$ in $\mathcal{G}$ is \concept{open} conditional on some set $C\subseteq V\setminus\{a,b\}$, if 
\begin{itemize}
	\item Every collider is in $\mathrm{an}_\mathcal{G}(C) = \bigcup_{i\in C}\mathrm{an}_\mathcal{G}(i)$ and
	\item No non-collider is in $C$.
\end{itemize}
\end{itemize}
\end{document}
